---
title: "SPRUCE Analysis"
author: "Autumn Pereira & Linnea Smith"
date: "2024-05-10"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data handling

## Environmental data
Downloaded from here: https://www.dropbox.com/scl/fo/o9zh9o3smmjg54l20aj2r/AEb8Unqsj0rJzAb24WRBDQA/WEW_Complete_Environ_20240328?rlkey=bg4byw9cebb6m2fjvoymlioyt&dl=0

Metadata available here: https://mnspruce.ornl.gov/datasets/spruce-whole-ecosystems-warming-wew-environmental-data-beginning-august-2015

These data were taken every half hour, every day, in each plot.
Before doing anything else, we have to combine the environmental data for all plots into a single dataset, limit it to our year of interest (2021).

There is a separate file for each plot, all downloaded into one folder. Because that's a lot of files/data, we have not included this raw data in our submission, but we can provide it or it can be downloaded at the above links if necessary.

First read in the files.
```{r}
# Vector of all files
env_filenames <- list.files("../Data/Raw/WEW_Complete_Environ_20220518/",full.names = T)

# Read in all as single list object
env_data_list <- lapply(env_filenames,read.csv)

# Extract plot numbers from file names
plot_nums <- stringr::str_extract(env_filenames,"(PLOT_\\d\\d).",group=1)
names(env_data_list) <- plot_nums

# Add column for plot number to each DF (will need for later combination into single DF)
for(i in 1:length(env_data_list)){
  env_data_list[[i]]$plot_number <- stringr::str_extract(names(env_data_list)[i],"\\d+")
}

```

Now we'll do some basic checks that all dataframes have the same structure, and then combine them into a single dataframe.
```{r}
# Column names of first dataset
plot4_env_cols <- colnames(env_data_list$PLOT_04)

# Compare to the rest
lapply(env_data_list[-1],FUN=function(x){
  if(all(colnames(x)==plot4_env_cols)){
    print("All columns equal")
  } else{
    print(which(colnames(x)!=plot4_env_cols))
  }
})
```

All datasets have the same columns!

Limit to just the year 2021 for all data (do this before combining into one DF to avoid having a ridiculously large DF)

```{r}
env_data_list_2021 <- lapply(env_data_list,FUN=function(x){
  return(x[x$Year==2021,])
})

# Check that the data was really reduced to only 2021
lapply(env_data_list_2021,FUN=function(x)unique(x$Year))
```

What are the dimensions of each dataset?

```{r}
# What's the dimensions of each dataset?
lapply(env_data_list_2021,FUN=dim)
```

Each plot has 17,520 observations for the year 2021, which is correct (48 observations per day, 365 days per year):

```{r}
48*365
```

Combine into one dataframe

```{r}
env_data <- data.table::rbindlist(env_data_list_2021)
```

Now let's check for irregularities in the data.

```{r}
summary(env_data)
```

Several columns have a few hundred NAs, but in a dataset with 210,240 observations this is neither surprising nor concerning. The deeper soil temperature measurements (TS_.100_A8, TS_.200_A9,TS_.100__C8, TS_.200__C9) have a couple thousand NAs -- maybe there were more equipment failures at lower depths, e.g. due to the higher pressure required to insert and remove equipment. It's interesting that this doesn't seem to have been the case in zone B, only in zones A and C. PREC_6 has 43,401 NAs, and seems to consist primarily of 0s. PREC_6 is the total mm of rainfall at 6 m on the central tower, at 30 minute intervals, so it makes sense why it's mostly 0s (it's not raining more often than it rains). I'm not sure why there are so many NAs, though.

Are the NAs mostly in the same rows? This could indicate equipment failures on specific days.
```{r}
# Get the total number of NAs in each row
row_nas <- rowSums(is.na(env_data))
summary(row_nas)
hist(row_nas)
```

Impossible to get a good look from this histogram. Narrow down to 0-20

```{r}
hist(row_nas,breaks=c(seq(0,20,1),length(row_nas)),xlim=c(0,20))
```

The majority of rows have 0 NAs. Remove all of these to get a better look
```{r}
row_nas <- row_nas[row_nas>0]
summary(row_nas)
hist(row_nas)
hist(row_nas,breaks=c(seq(0,91,1)))
```

Looks like most rows that have NAs only have 1, and almost all have fewer than 5.

```{r}
sum(row_nas == 91); sum(row_nas>50); sum(row_nas>20)
```

269 rows have 91 NAs; 471 more than 50; 491 more than 20.

```{r}
sum(row_nas < 20); sum(row_nas==1)
```

By contrast, 46,993 rows have fewer than 20 NAs; of these, 40,672 rows have only 1 NA. So no, the NAs are not mostly the same rows. However, this is probably because PREC_6 has so many more NAs than any other column. What happens if we do the same thing with PREC_6 removed?

```{r}
row_nas_nop <- rowSums(is.na(as.data.frame(env_data)[,c(which(colnames(env_data)!="PREC_6"))]))
summary(row_nas_nop)

row_nas_nop <- row_nas_nop[row_nas_nop>0]
summary(row_nas_nop)
hist(row_nas_nop,seq(1,max(row_nas_nop),1))
```

No, the vast majority are still singlet NAs. No further action needed - there are more than enough datapoints in each variable to make scattered individual NAs unimportant. Rows with more NAs are so rare as to be unimportant as well.

Now we'll check the data types of the different columns.

Year.Fraction, WS_10, PAR_2, and plot_number should all be numeric.

Year.Fraction

```{r}
head(env_data$Year.Fraction)
```

no immediately visible reason to not be numeric

```{r}
test <- as.numeric(env_data$Year.Fraction)
sum(is.na(test))
```

no NAs ==> no incorrect conversions from character to numeric. Convert to numeric
```{r}
env_data$Year.Fraction <- as.numeric(env_data$Year.Fraction)
```

WS_10

```{r}
head(env_data$WS_10,50)
test <- as.numeric(env_data$WS_10)
sum(is.na(test))==sum(is.na(env_data$WS_10))
```

Which columns became NA when converting to numeric?

```{r}
bad <- which(is.na(test) & !is.na(env_data$WS_10))
unique(env_data[bad,"WS_10"])
```
They're all just empty. It's correct to convert them to NA so we can make the conversion.

```{r}
env_data$WS_10 <- as.numeric(env_data$WS_10)
```

PAR_2

```{r}
head(env_data$PAR_2,50)
test <- as.numeric(env_data$PAR_2)
sum(is.na(test))==sum(is.na(env_data$PAR_2))
bad <- which(is.na(test) & !is.na(env_data$PAR_2))
unique(env_data[bad,"PAR_2"])
```

It looks like entries in the 1000s were sometimes written with a comma, causing them to register as character and not convert to numeric. Remove all commas in the column and then convert to numeric

```{r}
test<-env_data$PAR_2
test2<-gsub(",","",test)
test3 <- as.numeric(test2)
sum(is.na(test3))==sum(is.na(test))
```

Confirm the commas were the only problem

```{r}
env_data$PAR_2 <- gsub(",","",env_data$PAR_2)
env_data$PAR_2 <- as.numeric(env_data$PAR_2)

```


### Daily environmental data
Currently, we have the environmental data for every half hour of the whole year. Our response datasets contain data at a temporal resolution of 1 day, so we'll take daily averages of the environmental data.

Rename dataframe for clarity, make dataframe (was data.table)

```{r}
env_data_full <- data.frame(env_data)
```

Names of columns that remain relevant with daily data (timestamp, etc. become unnecessary) and environmental data except for precipitation

```{r}
daily_cols <- colnames(env_data_full)[c(1,6,8:(ncol(env_data_full)-2))]
```

Sum up precipitation by day and plot

```{r}
daily_precip <- aggregate(env_data_full[,"PREC_6"],
                            by=list(env_data_full$Day_of_Year,
                                    env_data_full$Plot),
                            FUN = sum,na.rm=T)
colnames(daily_precip) <- c("Day_of_Year","Plot","PREC_6")
```

Aggregate the other data by averaging over day and plot, removing NAs from the calculations

```{r}
env_data_daily <- aggregate(env_data_full[,daily_cols],
                            by=list(env_data_full$Day_of_Year,
                                    env_data_full$Plot),
                            FUN = mean,na.rm=T)
```

Group.1 and Group.2 should be extraneous - they are the same as Day_of_Year and Plot. Let's confirm this quickly.

```{r}
sum(env_data_daily$Group.1==env_data_daily$Day_of_Year) == nrow(env_data_daily)
sum(env_data_daily$Group.2==env_data_daily$Plot) == nrow(env_data_daily)
```
Yes, it's fine to remove those columns.

```{r}
env_data_daily$Group.1 <- NULL
env_data_daily$Group.2 <- NULL
```

Now we can merge this with the precipitation data.

```{r}
env_data_daily <- merge(env_data_daily,daily_precip,by=c("Day_of_Year","Plot"))
```

Let's make sure this all worked the way we wanted it to.

```{r}
table(env_data_daily$Plot)
```
Each of the 12 plots has 365 entries, one for every day of the year. This is correct.


We'll now do some checks on missing data in this aggregated environmental dataset. First, we'll check the overall structure of the new dataset.

```{r}
summary(env_data_daily)
```

Quite a few columns have 2 NAs -- assume this is a complete lack of data for that column on that day, but check in larger dataset to make sure.

```{r}
env_data_daily[which(is.na(env_data_daily$TS_0__A1)),c("Day_of_Year","Plot")]
env_data_full[which(env_data_full$Day_of_Year==226 &
                      env_data_full$Plot==10),"TS_0__A1"]
env_data_full[which(env_data_full$Day_of_Year==227 &
                      env_data_full$Plot==10),"TS_0__A1"]
```

As expected -- all NAs. 

Some columns (TS_.100__A8, TS_.200__A9, TS_.200__C9) have around 40 NAs. These are also
the ones that had thousands of NAs in the half-hourly dataset, so this isn't
surprising and doesn't need to be investigated further.

Save this daily data.
```{f}
#write.csv(env_data_daily,"./Data/Clean/dailymean_environmental_data_2021.csv",row.names = F)
```
This dataset and all others moving forward are included with the submission and on the GitHub in the folder "For_R_Markdown". We will comment out all calls to write.csv() so as not to overwrite anything, but include the calls nonetheless so that you can see the file names.

Now that we have usable environmental data, we'll have a think about what we want to use for our analysis and whittle it down to those variables.

### Reduce environmental data


(Note: the previous chunk was written by Linnea, the following by Autumn, with slightly different names for the dataframes, hence the changes to dataframe names.)

Set up for the next chunk.

```{r}
library(stringr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(gsubfn)

```

Let's take a look our column names.

```{r}
envr_orig <- read.csv("./dailymean_environmental_data_2021.csv")

envr <- envr_orig

cols <- colnames(envr)

cols[10:20]
```

These correspond to the soil temperature at different layers. For instance, TS_.5__A2 is the soil temperature at 5 cm below the soil surface in Zone A, while TS_.100__A8 is the temperature at 100cm below the soil surface in Zone A

A lot of the variables are of this type, reporting the soil temperature at different depths in different zones.

Let's convert this table into long form using the pivot_longer() function

```{r}
envr <- pivot_longer(envr, cols= 6:96, names_to= "measure")
```

That looks better. Let's now make sure to only include soil temperature measurements. We will start by separating out the variable names to have 

- a column of "measured" (the first two letters of the variable name, where TS= soil temp, RH= relative humidity, WS= wind speed, WD= wind direction)

- a column of depth

- a column of the "zone" it is in

```{r}
envr <- envr %>% mutate(var_measured = substr(measure, 1, 2))
envr <- envr %>% mutate(SD= str_extract(envr$measure, "_SD"))

envr <- subset(envr, var_measured == "TS")
envr <- subset(envr, is.na(SD) ==T)
envr <- envr[, -c(9)]

envr <- envr %>% mutate(depth = str_extract(measure, "_.(\\d+)", group = 1))
envr <- envr %>% mutate(zone = str_extract(envr$measure, "__([A-Z])", group = 1))
```

We have a few NAs in our dataset, which is from the measurements taken of the soil temperature at the hummock, which is a different microtopographical element of peatlands. We won't be using these datapoints, so we will remove these.

```{r}
envr <- na.omit(envr)
```

Great! The last thing to do is change the depth variable to a numeric

```{r}
envr$depth <- as.numeric(envr$depth)
```

Finally, we will make a table to check for anything odd

```{r}
table(envr$depth)
```

So it seems that a few entries may be missing for the 100 and 200 depths. Another thing to note is that the soil temperature was never taken at 1 cm, but these entries instead correspond to 0 cm depth, which is fine.

Let's table these by plot, so that we can see if there are certain plots missing the 100 and 200 cm depth data.

```{r}
table(envr$depth, envr$Plot)
```

Okay, so it seems that plots 4 and 21 are are the ones with variation in 100 and 200 cm depths compared to others. interestingly, we also see that plot 10 has less observations than the others. However, our other data was only taken on two different dates. So as long as we have records for those dates, that should be fine.

Those dates are 2021-06-23 and 2021-08-24. However, the dates in the environmental dataset are given as date of the year. 

2021-06-23 is the 174th day of the year, and 2021-08-24 is the 236th day of the year. 

```{r}
envr <- subset(envr, Day_of_Year == 174 | Day_of_Year == 236)
```

Let's table by depth and plot. We should have 2 measurements, one for each day

```{r}
table(envr$depth, envr$Plot)
```
Ah, okay. There seems to be an effect of the "zone" variable- one measurement was recorded for each "zone" (corresponds to a different point on the temperature sensor). We will take measurements from zone A, as this is the "center" position of the soil probe.

```{r}
envr <- subset(envr, zone == "A")
table(envr$depth, envr$Plot)
```
Great. That should suffice for now. 


## qPCR

Dataset download link: https://www.dropbox.com/scl/fo/8wspfe6m1o5efsgu38t7t/h/SPRUCE_qPCR_copy_numbers_2021.csv?dl=0

Data source with metadata: https://mnspruce.ornl.gov/datasets/spruce-quantitative-pcr-qpcr-of-microbial-gene-copy-numbers-2021-2022


```{r}
qpcr <- read.csv("./SPRUCE_qPCR_copy_numbers_2021.csv")
colnames(qpcr)
```
We really only care about the copies per gram dry weight. This corresponds to Bacteria_copy_dry and Archaea_copy_dry. Let's keep only these, and the details of the sampling location/date

```{r}
qpcr <- qpcr[,-c(13,15,16,17)]
```

Lastly, we will make a histogram of the Archaea_copy_dry and Bacteria_copy_dry to ensure we don't have outliers

```{r}
par(mfrow= c(1,2))
hist(qpcr$Archaea_copy_dry)
hist(qpcr$Bacteria_copy_dry)
```

These both seem okay, except for that there are two entries with the value at -9999, which is used in this dataset to represent an NA or some sort of error. Let's remove these entries

```{r}
qpcr <- subset(qpcr, Archaea_copy_dry >= 0 & Bacteria_copy_dry >= 0)
```


## Chloroform fumigation

Dataset download link: https://www.dropbox.com/scl/fo/wtphig601emenlyjbph8f/h/SPRUCE_CFE_MBC_MBN_2021_2022.csv?dl=0

Data source with metadata: https://mnspruce.ornl.gov/datasets/spruce-chloroform-fumigation-extraction-for-microbial-biomass-and-dissolved-organic-carbon

```{r}
chloroform <- read.csv("./SPRUCE_CFE_MBC_MBN_2021_2022.csv")
colnames(chloroform)
```

Here, we really only care about

- DN_unfumigated_soil (this is a measure of total N without the N stored in microbial biomass)
- MBN (microbial biomass nitrogen)
- DOC_unfumigated_soil (this is a measure of total DOC without the OC stored in microbial biomass)
- GWC: a measure of soil moisture

Let's remove the other variables (except those describing the sampling location and time) from this dataset

```{r}
chloroform <- chloroform[, -c(5:8,10:12,14,16,17,19)]
```

# Merge dataframes
## Formatting the envrironmental dataset

Merging the chloroform and qpcr dataframes won't be a big hassle, we just need to make sure that we are combining entries that have both the same date, plot, and depth. However, we have a bit more processing to do for the envr dataset, namely changing the dates and depths

Let's start by changing the dates to the same format as the qpcr dataset. Recall that 2021-06-23 is the 174th day of the year, and 2021-08-24 is the 236th day of the year.

```{r}
envr$date <- NA

for (i in 1:216){
  if (envr$Day_of_Year[i] == 174){
    envr$date[i] <- "2021-06-23"
  }
  else{
    envr$date[i] <- "2021-08-24"
  }
}
```

Let's make a variable combining date and site

```{r}
envr$datesite <- paste(envr$date, envr$Plot)
```

Cool. Now to work on the depth

Note that we have the following depth ranges for where qPCR and chloroform samples were taken

```{r}
unique(qpcr$Depth_range)
```

and temperature readings from these depths

```{r}
unique(envr$depth)
```

We don't have a one-to-one mapping between depth range of qpcr/chloroform measurements and the depth measurements. 

There are three options I can take

1. Average all of the temperatures, in order to get an estimate of the temperature at the midpoint (ex: average the measurements at 10 and 20 cm and report that as the value for the 10-20cm range): this could work, but there are a few issues, namely the 50-75, 75-100, 100-125, and 150-175 variables. 

2. Assign the temperature at the top layer for all layers: Similarly to the previous method, the 75-100, and 150-175 variables wouldn't have a value. I feel like we've already cut a lot of data, and I'm not sure if I'm happy cutting more.

3. A combination of 1 and 2: for the ranges where I can take the top layer measurement, I will use that, and for ranges where I cannot, I can take the average: this shoiuld work well, since I can assign:

50-75: measurement at 50 cm

75-100: average of 50cm and 100cm measurements

100-125: measurement at 100cm

150-175: average of 100cm and 200cm measurements.

We can test the validity of this by taking the average of the measurements at 0 and 10 cm, and comparing those to the actual values taken at 5 cm.

```{r}
envr_ranges <- matrix(ncol = 3, nrow = 0)
colnames(envr_ranges) <- c("datesite", "depth range", "temp")
date_site_combos <- unique(envr$datesite)

envr_0_10 <- subset(envr, depth == 1 | depth == 10)

for (i in 1:24){
  sub <- subset(envr_0_10, datesite == date_site_combos[i])
  envr_ranges <- rbind(envr_ranges, c(date_site_combos[i], "0-10", mean(sub$value)))
}

envr_ranges<- as.data.frame(envr_ranges)
```

```{r}
depth5 <- subset(envr, depth == 5)

comparison <- inner_join(depth5, envr_ranges, by = "datesite")
```

```{r}
plot(comparison$value, comparison$temp)

summary(lm(comparison$temp~comparison$value))
```

I think that this is okay. We don't have a 1-to-1 relationship, but this is the best way to keep all our data while making the best guess we can make.

Let's do this for the two depths that need it:

```{r}
envr_ranges <- matrix(ncol = 3, nrow = 0)
colnames(envr_ranges) <- c("datesite", "depth", "temp")
date_site_combos <- unique(envr$datesite)

envr_50_100 <- subset(envr, depth == 50 | depth == 100)
envr_100_200 <- subset(envr, depth == 100 | depth == 200)

for (i in 1:24){
  sub1 <- subset(envr_50_100, datesite == date_site_combos[i])
  envr_ranges <- rbind(envr_ranges, c(date_site_combos[i], "75-100", mean(sub1$value)))
  
  sub2 <- subset(envr_100_200, datesite == date_site_combos[i])
    envr_ranges <- rbind(envr_ranges, c(date_site_combos[i], "150-175", mean(sub2$value)))
}

envr_ranges<- as.data.frame(envr_ranges)
```

Cool. Lastly, we will add datesite data for other depths into the envr_ranges dataset to get something we can easily combine with the qPCR + chloroform datasets

```{r}
small_envr <- envr[,c(12, 9,7)]

colnames(small_envr) <- c("datesite", "depth", "temp")

complete_envr_small <- rbind(small_envr, envr_ranges)
complete_envr_small <- subset(complete_envr_small, depth != 5 & depth != 200)
```

```{r}
for (i in 1:216){
  if (complete_envr_small$depth[i] == 1){
    complete_envr_small$depth[i] <- "0-10"
  }
  else if (complete_envr_small$depth[i] == 10){
    complete_envr_small$depth[i] <- "10-20"  
  }
  else if (complete_envr_small$depth[i] == 20){
    complete_envr_small$depth[i] <- "20-30"
  }
  else if (complete_envr_small$depth[i] == 30){
    complete_envr_small$depth[i] <- "30-40"  
  }
  else if (complete_envr_small$depth[i] == 40){
    complete_envr_small$depth[i] <- "40-50"
  }
  else if (complete_envr_small$depth[i] == 50){
   complete_envr_small$depth[i] <- "50-75" 
  }
  else if (complete_envr_small$depth[i] == 100){
   complete_envr_small$depth[i] <- "100-125" 
  }
}
```

Cool! Let's move onto the other datasets

### Processing qPCR and Chloroform Datasets

Let's make a variable that combines the date and plot number.

```{r}
chloroform$datesite <- paste(chloroform$Sample_date, chloroform$Plot)
qpcr$datesite <- paste(qpcr$Sample_date, qpcr$Plot)
```

Now let's see what data overlaps between these datasets

```{r}
length(intersect(chloroform$datesite, qpcr$datesite))
length(unique(qpcr$datesite))
```

Okay, so it seems that we are losing ~12 samples by including qPCR results. However, we are including the most data that we can, since that is equal to the entire qPCR dataset.

Let's now make a variable that includes depth, site, and date. 

If we take a look at our depth variables, we can see that it while in the chloroform dataset it gives different ranges with a dash separating the top and bottom of the soil layer (in cm) (ex: 0-10, 10-20) in the qpcr dataset, it gives the same range with an underscore (ex: 0_10, 10_20). Let's change this so that they match.

```{r}
qpcr$depth2 <- NA

for (i in 1:214){
  split <- str_split(qpcr$Depth_range[i],"_")
  qpcr$depth2[i] <- paste0(split[[1]][1], "-", split[[1]][2])
}
```

Now, let's make a variable that includes date, site, and depth to make merging these dataframes easier.

```{r}
chloroform$datesitedepth <- paste(chloroform$datesite, chloroform$Depth)
qpcr$datesitedepth <- paste(qpcr$datesite, qpcr$depth2)
complete_envr_small$datesitedepth <- paste(complete_envr_small$datesite, complete_envr_small$depth)
```

Again, we will check the data loss

```{r}
length(intersect(chloroform$datesitedepth, qpcr$datesitedepth))
```

This corresponds to the entire qpcr dataset, so we are losing the least data possible with the datasets we have.

## Dealing with Replicates

Our last issue is that we have replicates in the chloroform and moisture datasets, but not the qpcr datasets. Let's average these to get a single datapoint for each date, site, and depth

```{r}
temp <- unique(chloroform$datesitedepth)

chloroform_means <- matrix(ncol = 6, nrow = length(temp))
colnames(chloroform_means) <- c(colnames(chloroform)[5:9], "datesitedepth")

for (i in 1:length(temp)){
  sub <- subset(chloroform, datesitedepth == temp[i])
  means <- colMeans(sub[, 5:9])
  chloroform_means[i,6] <- temp[i]
  for (j in 1:5){
    chloroform_means[i,j] <- means[j]
  }
}
```

## Merging Chloroform and qPCR Datasets

Finally, we will merge these datasets using inner_join() in the dplyr package. We will merge by our newly created "datesitedepth" variable.

Let's first convert our chloroform matrix to a dataframe
```{r}
chloroform_means_2 <- as.data.frame(chloroform_means)
```

and combine!

```{r}
data_pre <- inner_join(qpcr, chloroform_means_2, by = "datesitedepth")
data <- inner_join(data_pre, complete_envr_small, by = "datesitedepth")
```

Great! Now we have all of our datasets combined.

Finally, we will remove redundant variables from the site descriptors, to keep only date, plot, depth, temp experimental, and CO2_treatment, and the measured variables that we care about (described above)

```{r}
colnames(data)
```
```{r}
data <- data[, -c(1,2,4:7,10,14,22,23)]
```

Nice! Let's also rearrange the data rows so they're a bit nicer

```{r}
data <- data[, c(1:4,7,8,5,6,9:14)]
```

Finally, let's add precipitation back in
```{r}
# Convert day of year to date
envr_orig$Sample_date <- as.character(as.Date(envr_orig$Day_of_Year,origin="2020-12-31"))

# Grab precip data
data <- merge(data,envr_orig[,c("Sample_date","Plot","PREC_6")],
                 by=c("Sample_date","Plot"),all.x=T,all.y=F)
```

and write to csv

```{r}
#write.csv(data,"complete_combined_spruce_data.csv" )
```

# Exploratory analysis

## Covariance matrix

Before we do any proper analyses, let's check the distributions and covariances of our variables. This will tell us if we need to transform any variables, and will also let us check for multicollinearity between variables.

Rename to descriptive DF name and investigate structure
```{r}
spruce <- data
str(spruce)
```
Some of these are character and should be numeric.
```{r}
spruce$GWC <- as.numeric(spruce$GWC)
spruce$DN_unfumigated_soil <- as.numeric(spruce$DN_unfumigated_soil)
spruce$DOC_unfumigated_soil <- as.numeric(spruce$DOC_unfumigated_soil)
spruce$MBN <- as.numeric(spruce$MBN)
spruce$MBC <- as.numeric(spruce$MBC)
spruce$temp <- as.numeric(spruce$temp)
str(spruce)
summary(spruce)
```


Make a covariance matrix:
```{r}
library(GGally)

# Make covariance matrix
ggpairs(subset(spruce, select = -c(Sample_date,Plot,datesitedepth,Temp_experimental,CO2_treatment,depth2)),
                lower=list(continuous="smooth_lm")) + theme_light()
```

Looks like there's quite an outlier in MBN. Let's see what it looks like if we limit MBN to values under 100.

```{r}
ggpairs(subset(spruce, MBN < 100, 
                       select = -c(Sample_date,Plot,datesitedepth,Temp_experimental,CO2_treatment,depth2)),
                lower=list(continuous="smooth_lm")) + theme_light()
```

That makes it way better.

We also saw above that there are some negative values in MBN and MBC. This is physically impossible. Looking at the dataset description we see that due to "heterogeneity between the two replicate samples" used to calculate MBN and MBC, there are a few negative values. Since negative values are nonsensical, we'll change these to 0 to reflect that they have a very small microbial biomass.

```{r}
spruce[which(spruce$MBN<0),"MBN"] <- 0
spruce[which(spruce$MBC<0),"MBC"] <- 0
```

While we're at it, we'll also remove the row with the MBN outlier, since this value is unusable and we want to use complete cases only.
```{r}
spruce[which(spruce$MBN>100),"MBN"] <- NA
spruce <- na.omit(spruce)
```

Next we'll remove the ambient temperature treatments from the dataset. To see the justification for this, look at the boxplots in the exploratory analysis section. We also will remove the row of the MBN 
```{r}
spruce <- spruce[-which(spruce$Temp_experimental=="Amb"),]
```

Now we can also make experimental temperature a numeric variable, since we no longer have to distinguish the +0 degrees Celcius treatment from the ambient treatment. However, we also want to keep the column where it's a factor, for the PCA.
```{r}
spruce$Temp_experimental_num <- as.numeric(spruce$Temp_experimental)
```


Finally, looking at the distributions of the variables (the diagonal of the covariance matrix), it looks like MBN, MBC, bacterial copy number, and archaeal copy number are all right skewed. Let's log transform these.

```{r}
spruce_log_scale <- spruce

# Variables to transform
to_transform <- c("Bacteria_copy_dry","Archaea_copy_dry","MBN","MBC")

# Check for 0s
apply(spruce_log_scale[,to_transform],2,FUN=min,na.rm=T)
```

Add 1 to MBC and MBN because they include 0.
```{r}
spruce_log_scale$MBN <- spruce_log_scale$MBN + 1
spruce_log_scale$MBC <- spruce_log_scale$MBC + 1
```

Perform log transformation.
```{r}
spruce_log_scale[,to_transform] <- apply(spruce_log_scale[,to_transform],2,log)
```

Finally, let's scale all values.
```{r}
# Vector of numeric columns
to_scale <- unlist(lapply(spruce_log_scale, is.numeric), use.names = FALSE)

# Double check these are only columns we want
colnames(spruce_log_scale)[to_scale]
```

Whoops, Plot shouldn't be numeric -- we aren't going to be using it in any of our transformations or analyses. We'll make it a character and check the variables to be scaled again.
```{r}
# Make plot into a character
spruce_log_scale$Plot <- as.character(spruce_log_scale$Plot)

# Redo the vector of numeric columns
to_scale <- unlist(lapply(spruce_log_scale, is.numeric), use.names = FALSE)
colnames(spruce_log_scale)[to_scale]
```

Those are all the columns we want to scale - let's actually perform the scaling now.
```{r}
# Scale columns
spruce_log_scale[,to_scale] <- scale(spruce_log_scale[,to_scale])

```

Save this
```{r}
# write.csv(spruce_log_scale,"./Data/Clean/complete_spruce_logged_scaled.csv",row.names = F)
```

# Exploratory analysis
## ANOVAs

The bulk of our exploratory analysis was done in the Shiny app. Below is the base R code we based the Shiny app on, as a worked example of how the code functions. For testing, we used microbial biomass nitrogen (MBN) as the predicted variable and depth and experimental temperature as explanatory variables in a two-way ANOVA with interaction effects.

Replace dash with underscore in depth ranges so the names don't confuse the modeling

```{r}
spruce$depth2 <- gsub("-","_",spruce$depth2)
```

Select variable for ANOVA

```{r}
lhs <- "MBN"
```

Select explanatory variable(s)

```{r}
x_axis <- "depth2"
fill_var <- "Temp_experimental"
```

We want to log-transform the variables that need it, but we don't want to use the scaled values, so we'll just perform the log transformation here.

```{r}
# Initialize dataframe for log values
spruce_log <- spruce

# Check for 0s
apply(spruce_log[,to_transform],2,FUN=min,na.rm=T)

# Add 1 to MBC and MBN because they include 0
spruce_log$MBN <- spruce_log$MBN + 1
spruce_log$MBC <- spruce_log$MBC + 1

# Transform
spruce_log[,to_transform] <- log(spruce_log[,to_transform])
```

Now we'll run the ANOVA.

```{r}
# Create formula
aov_formula <- paste0(lhs,"~",paste(x_axis,fill_var,sep ="*"))

# Run ANOVA
aov_object <- aov(formula(aov_formula), data=spruce_log)
summary(aov_object)
```

We'll also run a TukeyHSD test to see what levels are significantly different from one another.

```{r}
tukey <- TukeyHSD(aov_object)
```

## Boxplots

Now we'll make our boxplot. We'll want to use letters to designate what's significantly different from each other, so let's first extract the p-values from the TukeyHSD test.

```{r}
tukey_p <- lapply(tukey,FUN=function(x){
  y <- x[,4]
  names(y) <- rownames(x)
  return(y)
})
```

We'll get the letter assignments from the p-values using the multcompView library. Since we have a two-way ANOVA, one of which is on the x-axis and one of which dictates the color, we have to variables we need to assign letters for. One goes on the x-axis. We'll add the letters for the color-coding variable to the legend, to keep things legible.

```{r}
library(multcompView)
x_letters <- multcompLetters(tukey_p[[x_axis]])$Letters
x_letters <- data.frame(level = names(x_letters),letter = x_letters)

fill_letters <- multcompLetters(tukey_p[[fill_var]])$Letters

for_legend <- paste(names(fill_letters)," (",fill_letters,")",sep="")
names(for_legend) <- names(fill_letters)

```

We also checked for interactions between variables. There are too many levels to depict these on the boxplots, so if there are interactions, we'll print them as a separate table.

First, we have a failsafe to make sure we're only checking for interactions in cases where we ran a model with interactions.

```{r}
int_exists <- tryCatch(
  expr = (length(tukey[[3]]) > -1),
  error = function(e) {
    return(F)
  }
)
```

Next, if we ran a model with interactions, we'll check to see if there were any significant interactions.

```{r}
if (int_exists) {
  int <- which(tukey_p[[3]] < 0.05)
}
```

Finally, if there were any significant interactions, print them as a table
```{r}
if(length(int) > 0){
  interaction_table <- tukey[[3]][int,]
  interaction_table <- as.data.frame(interaction_table)
  colnames(interaction_table) <- c("Difference","Lower","Upper","Adjusted p-value")
}
interaction_table
```

Now we can make a boxplot.

```{r}
library(ggplot2)
ggplot(data=spruce_log)+
  geom_boxplot(aes(x=.data[[x_axis]],y=.data[[lhs]],fill=.data[[fill_var]]))+
  theme_classic() +
  geom_text(data=x_letters,aes(x=level,y=(max(spruce_log[[lhs]])+max(spruce_log[[lhs]])/10)),label=x_letters$letter) +
  scale_fill_discrete(labels=for_legend)


```


# Multiple Regression

We will perform two multiple regressions. We will try to use the full suite of measured variables to predict what treatment each observation was taken from. In other words, we will try to recover the CO2 treatment and the temperature treatment from the observed data. We will perform one regression for CO2 treatment and one regression for temperature treatment.


We saw in the covariance matrices that none of the variables have a correlation coefficient above 0.6, so we are not concerned about multicollinearity.

First load the libraries necessary.

```{r}
library(MASS)
library(ggplot2)
library(car)
```


## CO2 Treatments

We will make 2 smaller data frames containing only our predictors and response variable

```{r}
d <- spruce_log
CO2 <- d[, c(4, 7:14)]
temp <- d[, c(3,7:14)]
```

And we will code the CO2 variable as binary, where 1= elevated CO2 and 0 = ambient CO2

```{r}
CO2$CO2 <- NA

for (i in 1:177){
  if (CO2$CO2_treatment[i] == "E"){
    CO2$CO2[i] = 1 }
  else{
   CO2$CO2[i] = 0 
  }
}

CO2 <- CO2[, -c(1)]
```

We will fit a binomial GLM to the experimental CO2, and use stepwise AIC in both directions to select the most appropriate model.

```{r}
CO2_model <- glm(CO2 ~.,family=binomial(link='logit'),data=CO2)

CO2_step_model <- stepAIC(CO2_model, direction = "backward")
summary(CO2_step_model)

final_CO2_model <- glm(formula = CO2 ~ Bacteria_copy_dry + GWC + temp, family = binomial(link = "logit"), data = CO2)
```

Great, so the best model allowing us to parse out our ambient and elevated CO2 treatments includes

- Log bacterial copy number
- Gravimetric water content
- Temperature

The assumptions of logistic regression that we haven't checked is that there is a linear relationship between logit(CO2) and our response variables. We can test this using a Box-Tidwell test.

```{r}
CO2$logits <- final_CO2_model$linear.predictors
```

```{r}
plot(CO2$logits, CO2$Bacteria_copy_dry)
plot(CO2$logits, CO2$GWC)
plot(CO2$logits, CO2$temp)
```

Although there is a lot of spread, there is prevalence of a linear relationship from these plots, so the model is valid.

## Temperature treatments

Let's do the same for temperature. Here we will fit just a linear model since the temperatures are distributed linearly.

Based on the regression plots in the lower triangle of the covariance matrices, it seems that the independent variables are more or less linearly related to experimental temperature, so that satisfies that concern.

```{r}
temp_model <- lm(Temp_experimental ~., data = temp)
# Stepwise regression model
temp_step_model <- stepAIC(temp_model, direction = "backward", 
                      trace = TRUE)
summary(temp_step_model)
```

Great! So the best model in this case includes

- log bacteria copy number
- archaea copy number
- gravimetric water content
- dissolved nitrogen
- microbial biomass nitrogen
- dissolved organic carbon
- temperature

Let's look at the model assumptions

```{r}
plot(lm(formula = Temp_experimental ~ Bacteria_copy_dry + Archaea_copy_dry + 
    GWC + DN_unfumigated_soil + MBN + DOC_unfumigated_soil + 
    temp, data = temp))
```
The residuals vs fitted do look a bit odd, possibly because while our response variable takes values linearly distributed betweeen [0,9], it is categorical. Otherwise, the plots look acceptable. Otherwise, this follows the model assumptions quite well.

# Structural Equation Modeling

## Test model assumptions

Our hypothesized SEM is detailed in the paper. We'll therefore jump right into testing the model assumptions of the underlying linear relationships of those hypotheses.

Dissolved organic carbon
```{r}
DOC_lm <- lm(DOC_unfumigated_soil ~ depth2 + CO2_treatment + depth2, data=spruce_log_scale, na.action=na.omit)
plot(DOC_lm)
```

This has a bit of a skew, but not so much that I'm concerned about it.

Dissolved nitrogen

```{r}
DN_lm <- lm(DN_unfumigated_soil ~ depth2 + Temp_experimental, data=spruce_log_scale, na.action=na.omit)
plot(DN_lm)
```

Pretty much the same behavior as DOC -- I'm fine with this.

Soil temperature

```{r}
temp_lm <- lm(temp ~ Temp_experimental + depth2,
              data=spruce_log_scale, na.action=na.omit)
plot(temp_lm)
```

This is fine.

Gravimetric water content

```{r}
GWC_lm <- lm(GWC ~ depth2 + PREC_6, data=spruce_log_scale, na.action=na.omit)
plot(GWC_lm)
```

This is mostly fine, but definitely has a skew in the Q-Q plot. Let's check if it's better when log transformed.

```{r}
GWC_log_lm <- lm(log(GWC) ~ depth2 + PREC_6, data=spruce_log_scale, na.action=na.omit)
plot(GWC_log_lm)
```

It actually skews it worse. Stick to the untransformed data.

Bacteria copy number

```{r}
BCN_lm <- lm(Bacteria_copy_dry ~ DOC_unfumigated_soil + DN_unfumigated_soil + temp + GWC,
             data=spruce_log_scale, na.action=na.omit)
plot(BCN_lm)
```

The bacteria copy number is already log transformed, and it looks good. Points 108 and 135 are on the edge of being outliers, but we don't have methodological knowledge (such as those particular measurements being messed up on that particular day) to justify removing them, so we'll leave them alone.

Archaea copy number 

```{r}
ACN_lm <- lm(Archaea_copy_dry ~ DOC_unfumigated_soil + DN_unfumigated_soil + temp + GWC,
             data=spruce_log_scale, na.action=na.omit)
plot(ACN_lm)
```

Archaeal copy number is also already log transformed. I'm satisfied with how it looks.

Microbial biomass nitrogen

```{r}
MBN_lm <- lm(MBN ~ DN_unfumigated_soil + temp + GWC,
             data=spruce_log_scale, na.action=na.omit)
plot(MBN_lm)
```

This is still fairly skewed, but it's already log-transformed, so it's not going to get any better. It's likely so skewed because of the right-skew that results from having multiple 0s that become 1 when we add 1 to log-transform.

Microbial biomass carbon

```{r}
MBC_lm <- lm(MBC ~ DOC_unfumigated_soil + temp + GWC,
             data=spruce_log_scale, na.action=na.omit)
plot(MBC_lm)
```

The platonic ideal of a Q-Q plot. MBC is also already log transformed and it looks good.

## SEM

Now that we're satisfied that all model assumptions are met, we can perform the acutal SEM.

```{r, R.options = list(width = 150)}
library(piecewiseSEM)

spruce_psem <- psem(
  
  # Intermediate layer: DOC, DN, temperature, GWC
  lm(DOC_unfumigated_soil ~ depth2 + Temp_experimental_num + CO2_treatment, data=spruce_log_scale, na.action=na.omit),
  lm(DN_unfumigated_soil ~ depth2 + Temp_experimental_num, data=spruce_log_scale, na.action=na.omit),
  lm(temp ~ Temp_experimental_num + depth2,
     data=spruce_log_scale, na.action=na.omit),
  lm(GWC ~ depth2 + PREC_6, data=spruce_log_scale, na.action=na.omit),
  
  # Predicted variables layer: Bacteria and Archaea copy numbers, MBN, MBC
  lm(Bacteria_copy_dry ~ DOC_unfumigated_soil + DN_unfumigated_soil + temp + GWC,
     data=spruce_log_scale, na.action=na.omit),
  lm(Archaea_copy_dry ~ DOC_unfumigated_soil + DN_unfumigated_soil + temp + GWC,
     data=spruce_log_scale, na.action=na.omit),
  lm(MBN ~ DN_unfumigated_soil + temp + GWC,
     data=spruce_log_scale, na.action=na.omit),
  lm(MBC ~ DOC_unfumigated_soil + temp + GWC,
     data=spruce_log_scale, na.action=na.omit)
  
)

suppressWarnings(summary(spruce_psem, .progressBar = FALSE))
```

Fisher's C and the chi-squared tests tell us that this model does not fit. Based on the t-rule, we can afford to add a maximum of 12 additional relationships without oversaturating our model, so let's look at the tests of directed separation (d-sep tests) above and see if there are any relationships there that are currently missing in our model, and that make biological sense to add in.

MBN and MBC could both have significant relationships with depth and experimental temperature. We expected these relationships to be primarily indirect, i.e. mediated by intermediate variables, within the SEM, but it looks like the direct relationships between these variables are also important. This makes biological sense so I'll add them in.

```{r, R.options = list(width = 150)}
spruce_psem <- update(spruce_psem,
       MBN ~ DN_unfumigated_soil + temp + GWC + depth2 + Temp_experimental_num,
       MBC ~ DOC_unfumigated_soil + temp + GWC + depth2 + Temp_experimental_num)
suppressWarnings(summary(spruce_psem, .progressBar = FALSE))
```

The model itself still doesn't fit, but the R-squareds for MBN and MBC are much higher now! We still have 8 relationships remaining that we could add, so let's take another look at the d-sep tests. It looks like DOC and DN could be significantly affected by soil temperature. I'm willing to believe this could be the case, so I'll add it into my model.


```{r, R.options = list(width = 150)}
spruce_psem <- update(spruce_psem,
                      DOC_unfumigated_soil ~ depth2 + Temp_experimental_num + CO2_treatment + temp,
                      DN_unfumigated_soil ~ depth2 + Temp_experimental_num + temp)
suppressWarnings(summary(spruce_psem, .progressBar = FALSE))

```

At this point, there are only 6 more relationships that could be added, and I feel I'm running the risk of moving the model too far away from my original hypotheses in search of significant p-values, so I'll stop here and draw the conclusion that SEM is not suitable for this dataset.

## Create hypothesized path diagram

Although we ultimately can't draw conclusions from our SEM, we still need a diagram for our initial hypothesis. This is easiest to create from an already-extant SEM using built in functions from piecewiseSEM and DiagrammeR.

### Numeric dataset

To create a diagram, we need to have all-numeric data, so let's make all of our variables numeric.

```{r}
spruce_num <- spruce
```

Temperature treatment

```{r}
unique(spruce_num$Temp_experimental)
spruce_num$Temp_experimental <- as.numeric(spruce_num$Temp_experimental)
```

CO2 treatment

```{r}
unique(spruce_num$CO2_treatment)
```

A is ambient, and E is elevated 500 ppm above ambient. Change these to 0 and 1.

```{r}
spruce_num[which(spruce_num$CO2_treatment=="A"),"CO2_treatment"] <- 0
spruce_num[which(spruce_num$CO2_treatment=="E"),"CO2_treatment"] <- 1
spruce_num$CO2_treatment <- as.numeric(spruce_num$CO2_treatment)
```

Depth

```{r}
unique(spruce_num$depth2)
```

Take the top of the range (i.e. the smaller number) as representative of the soil range.

```{r}
spruce_num$depth2 <- stringr::str_extract(spruce_num$depth2,"(\\d+)_",group=1)
spruce_num$depth2 <- as.numeric(spruce_num$depth2)
```

Log-transform and scale numeric data

```{r}
spruce_num_log_scale <- spruce_num

# Variables to transform
to_transform <- c("Bacteria_copy_dry","Archaea_copy_dry","MBN","MBC")

# Check for 0s
apply(spruce_num_log_scale[,to_transform],2,FUN=min,na.rm=T)
```

Add 1 to MBC and MBN because they include 0

```{r}
spruce_num_log_scale$MBN <- spruce_num_log_scale$MBN + 1
spruce_num_log_scale$MBC <- spruce_num_log_scale$MBC + 1

# Log-transformation
spruce_num_log_scale[,to_transform] <- log(spruce_num_log_scale[,to_transform])
```

Scale all numerical values

```{r}
# Vector of numeric columns
to_scale <- unlist(lapply(spruce_num_log_scale, is.numeric), use.names = FALSE)

# Make plot into a character
spruce_num_log_scale$Plot <- as.character(spruce_num_log_scale$Plot)

# Redo the vector of numeric columns
to_scale <- unlist(lapply(spruce_num_log_scale, is.numeric), use.names = FALSE)

# Scale columns
spruce_num_log_scale[,to_scale] <- scale(spruce_num_log_scale[,to_scale])

# Remove any NA rows (just in case)
spruce_num_log_scale_noNA <- na.omit(spruce_num_log_scale)
```

Now we'll fit our final model from before, with this numerical dataset.

```{r}
spruce_num_psem <- psem(
  
  # Intermediate layer: DOC, DN, temperature, GWC
  lm(DOC_unfumigated_soil ~ depth2 + Temp_experimental + CO2_treatment + temp, data=spruce_num_log_scale, na.action=na.omit),
  lm(DN_unfumigated_soil ~ depth2 + Temp_experimental + temp, data=spruce_num_log_scale, na.action=na.omit),
  lm(temp ~ Temp_experimental + depth2,
     data=spruce_num_log_scale, na.action=na.omit),
  lm(GWC ~ depth2 + PREC_6, data=spruce_num_log_scale, na.action=na.omit),
  
  # Predicted variables layer: Bacteria and Archaea copy numbers, MBN, MBC
  lm(Bacteria_copy_dry ~ DOC_unfumigated_soil + DN_unfumigated_soil + temp + GWC,
     data=spruce_num_log_scale, na.action=na.omit),
  lm(Archaea_copy_dry ~ DOC_unfumigated_soil + DN_unfumigated_soil + temp + GWC,
     data=spruce_num_log_scale, na.action=na.omit),
  lm(MBN ~ DN_unfumigated_soil + temp + GWC + depth2 + Temp_experimental,
     data=spruce_num_log_scale, na.action=na.omit),
  lm(MBC ~ DOC_unfumigated_soil + temp + GWC + depth2 + Temp_experimental,
     data=spruce_num_log_scale, na.action=na.omit)
  
)
```

Out of curiosity, let's see if this performs any better than the SEM that had some categorical data.

```{r, R.options = list(width = 150)}
suppressWarnings(summary(spruce_num_psem, .progressBar = FALSE))
suppressWarnings(summary(spruce_num_psem, .progressBar = FALSE)$R2)
suppressWarnings(summary(spruce_psem, .progressBar = FALSE)$R2)
```

In fact, this is worse than the categorical SEM for explaining all endogenous variables. Let's compare AICs.

```{r}
AIC_psem(spruce_psem)
AIC_psem(spruce_num_psem)
```
Categorical is better than numeric. Still, we can use it to get the structure of our plot.

### Plotting

For whatever reason, the function piecewiseSEM::plot.psem() doesn't import for me from the library, so I copied it from the package's GitHub page and load it in manually.

```{r}
source("./plot_psem.R")
library(DiagrammeR)
```

#### Hypothesized path diagram

First, we'll get the structure of our original hypothesis SEM, not the one we ended up with, to create our hypothesis diagram.

```{r}
spruce_num_psem_init <- psem(
  
  # Intermediate layer: DOC, DN, temperature, GWC
  lm(DOC_unfumigated_soil ~ depth2 + Temp_experimental + CO2_treatment, data=spruce_num_log_scale, na.action=na.omit),
  lm(DN_unfumigated_soil ~ depth2 + Temp_experimental, data=spruce_num_log_scale, na.action=na.omit),
  lm(temp ~ Temp_experimental + depth2,
     data=spruce_num_log_scale, na.action=na.omit),
  lm(GWC ~ depth2 + PREC_6, data=spruce_num_log_scale, na.action=na.omit),
  
  # Predicted variables layer: Bacteria and Archaea copy numbers, MBN, MBC
  lm(Bacteria_copy_dry ~ DOC_unfumigated_soil + DN_unfumigated_soil + temp + GWC,
     data=spruce_num_log_scale, na.action=na.omit),
  lm(Archaea_copy_dry ~ DOC_unfumigated_soil + DN_unfumigated_soil + temp + GWC,
     data=spruce_num_log_scale, na.action=na.omit),
  lm(MBN ~ DN_unfumigated_soil + temp + GWC,
     data=spruce_num_log_scale, na.action=na.omit),
  lm(MBC ~ DOC_unfumigated_soil + temp + GWC,
     data=spruce_num_log_scale, na.action=na.omit)
  
)


diagram <- plot.psem(spruce_num_psem_init,digits=0, return=T, layout="tree")
```

We now have a list item describing the different aspects of the path diagram. Let's go in and manually edit these to make the diagram look nicer.

```{r}
# Readable labels
diagram$nodes_df$label <- c("DOC","DN","Soil\ntemp.","GWC","Bacteria",
                            "Archaea","MBN","MBC","Depth","Exp.\ntemp.",
                            "CO2\nlevel","Precip.")

# Different shapes for exogenous and endogenous variables
diagram$nodes_df$shape <- c(rep("oval",times=8),rep("rectangle",times=4))

# Change layout
diagram$global_attrs[1,2] <- "dot"

# Make nodes a little wider
diagram$global_attrs[8,2] <- 0.6

# Make all arrows solid
diagram$edges_df$style <- "solid"

# Change colors of edges
diagram$edges_df$color <- RColorBrewer::brewer.pal(n = 8, name = "Dark2")[diagram$edges_df$to]

# Change colors of boxes
diagram$nodes_df$color <- c(RColorBrewer::brewer.pal(n = 8, name = "Dark2"),rep("black",times=4))

# Remove edge numbers
diagram$edges_df$label <- NA

# Check how it looks
render_graph(diagram)

# Save diagram
#export_graph(diagram,"./Plots/hypothesis_sem.png",width = 1200, height = 800)
```

#### Final path diagram

Now, more for the sake of having it than anything else, we'll create a path diagram for the final numerical SEM and export it for loading into Shiny.

```{r}
# Get diagram structure from final model structure, made using numeric dataset
diagram <- plot.psem(spruce_num_psem,digits=2, return=T, layout="tree")

## Modify the nodes DF
# Readable labels
diagram$nodes_df$label <- c("DOC","DN","Soil\ntemp.","GWC","Bacteria",
                            "Archaea","MBN","MBC","Depth","Exp.\ntemp.",
                            "CO2\nlevel","Precip.")

# Different shapes for exogenous and endogenous variables
diagram$nodes_df$shape <- c(rep("oval",times=8),rep("rectangle",times=4))

# Change layout
diagram$global_attrs[1,2] <- "dot"

# Make nodes a little wider
diagram$global_attrs[8,2] <- 0.6

# Change colors of edges
diagram$edges_df$color <- RColorBrewer::brewer.pal(n = 8, name = "Dark2")[diagram$edges_df$to]

# Change colors of boxes
diagram$nodes_df$color <- c(RColorBrewer::brewer.pal(n = 8, name = "Dark2"),rep("black",times=4))

# Convert node ID numbers to descriptions so the mouseover will be descriptive
node_ids <- diagram$nodes_df$label
names(node_ids) <-  diagram$nodes_df$id
diagram$nodes_df$id <-diagram$nodes_df$label

diagram$edges_df$from <- node_ids[diagram$edges_df$from]
diagram$edges_df$to <- node_ids[diagram$edges_df$to]

# See how it all looks
render_graph(diagram)
```

Now we'll convert this to graphViz/DOT format, which is compatible with shiny, and we'll save this as a text document so that it can be imported into the shiny app and parsed by DiagrammeR there.

```{r}
dot <- generate_dot(diagram)

# Save DOT output for use in graphViz/Shiny
#writeLines(dot,"./Plots/dot_psem.txt")

# Also save the PSEM summary
#saveRDS(summary(spruce_num_psem),file = "./Analyses/psem_numeric_summary.RDS")

```
